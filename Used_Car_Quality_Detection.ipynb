{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Used Car Quality Detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGHdfmmz7mLE088dE1gooN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hargurjeet/MachineLearning/blob/master/Used_Car_Quality_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdpz77FmwQRF"
      },
      "source": [
        "# **Car Quality Detection by XBboost and Random Forests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK3oibLQwrvy"
      },
      "source": [
        "Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. In this notebook I implement two of the most known machine learning algorthim to predict the quality of a car."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23u3dNcxR5i"
      },
      "source": [
        "# **Table Of Contents**<a name=\"top\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  1. [About the Dataset](#1)\n",
        "  2. [Loading the dataset Preprocessing](#2)\n",
        "  3. [Expainatory Data Analysis](#3)\n",
        "  4. [Feature Engineering](#4)\n",
        "  4. [Data Cleaning and Pipelines Implementation](#4)\n",
        "  5. [Implemneting Random Forest](#5)\n",
        "  6. [Implemneting XBboost Forest](#6)\n",
        "  7. [Hyperparamterization](#7)\n",
        "  8. [Testing the models on sample dataset](#8)\n",
        "  9. [Summary](#9)\n",
        "  10. [Future Work](#10)\n",
        "  11. [References](#11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0AddVA8xhJi"
      },
      "source": [
        "# <font color=\"darkslateblue\" size=+2.0><b>1: AboutDataset</b></font> <a name=\"1\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2kf9DbyAWN"
      },
      "source": [
        "One of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. The auto community calls these unfortunate purchases \"kicks\".\n",
        "\n",
        "Kicked cars often result when there are tampered odometers, mechanical issues the dealer is not able to address, issues with getting the vehicle title from the seller, or some other unforeseen problem. Kick cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle.\n",
        "\n",
        "Modelers who can figure out which cars have a higher risk of being kick can provide real value to dealerships trying to provide the best inventory selection possible to their customers.\n",
        "\n",
        "The challenge of this competition is to predict if the car purchased at the Auction is a Kick (bad buy).\n",
        "\n",
        "- The challenge of this competition is to predict if the car purchased at the Auction is a good / bad buy.\n",
        "- All the variables in the data set are defined in the file Carvana_Data_Dictionary.txt \n",
        "- The data contains missing values \n",
        "- The dependent variable (IsBadBuy) is binary (C2)\n",
        "- There are 32 Independent variables (C3-C34)\n",
        "- The data set is split to 60% training and 40% testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx_dbCpoyA_F"
      },
      "source": [
        "# <font color=\"black\" size=+2.0><b>2: Loading the dataset Preprocessing</b></font> <a name=\"2\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JW0cqebc_CE"
      },
      "source": [
        "Downloading all the required python packages to get started, also imported all the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4TWeEDUB3V"
      },
      "source": [
        "!pip install numpy pandas matplotlib seaborn --quiet\n",
        "!pip install jovian opendatasets xgboost graphviz lightgbm scikit-learn xgboost lightgbm --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UzdMAEUIYO"
      },
      "source": [
        "#importing dataset\n",
        "import os\n",
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "#For Missing Value and Feature Engineering\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator\n",
        "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "\n",
        "import time\n",
        "\n",
        "#for visualization\n",
        "import seaborn as sns\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "pd.set_option(\"display.max_rows\", 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SizpSP4Jd4k6"
      },
      "source": [
        "To download the dataset from Kaggle, I use the library **od**.\n",
        "\n",
        "To connect to kaggle enter user Kaggle username name and API key. \n",
        "\n",
        "Please read though this article to understand the process of getting your API key from kaggle.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfCdVqEMUJ97"
      },
      "source": [
        "od.download('https://www.kaggle.com/c/DontGetKicked/data?select=test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bb9TUT0e8tv"
      },
      "source": [
        "Below are the list of files downloaded. You can access all the downloaded filename using os.listdir(<'downloaded folder name'>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_4ukX7TUPev"
      },
      "source": [
        "os.listdir('DontGetKicked')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ubhOvfXn0"
      },
      "source": [
        "I use pandas to read the train and test data sets. Here I access few records of training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QafC5UF9Ubfk"
      },
      "source": [
        "train=pd.read_csv('/content/DontGetKicked/training.csv') \n",
        "test= pd.read_csv('/content/DontGetKicked/test.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D6YDR8DyA6I"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>3: Expainatory Data Analysis</b></font> <a name=\"3\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj3jC1M2p3HO"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>4: Feature Engineering</b></font> <a name=\"4\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1GOBrqVIEz"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvNQ3n8CGnU"
      },
      "source": [
        "Before performing the feature engineering, let us first check the data quality and identify null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxHWn2xrBj2r"
      },
      "source": [
        "print(train.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRPFiHBB_6o"
      },
      "source": [
        "print(test.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWqQjR_CxT6"
      },
      "source": [
        "**Checking Duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX60-blFCwzw"
      },
      "source": [
        "train[train.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZtk0i-ED0kT"
      },
      "source": [
        "test[test.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buC5pKQhFNpP"
      },
      "source": [
        "As we see above, many column contains null values. It is important for us to understand the relevant columns that would help model to better generalize.\n",
        "Hence following columns seems not relevant that are requied to be passed to the model\n",
        "- PurchDate (Date might not be relevant by Year would be)\n",
        "- WheelTypeID\n",
        "- Model\n",
        "- Trim \n",
        "- SubModel\n",
        "- Make\n",
        "- VNZIP1\n",
        "- VNST\n",
        "- Color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqE8VSpOf5-G"
      },
      "source": [
        "Here I create few addtional column using the existing column to drive some addtional features from the datasets.\n",
        "\n",
        "The additional features help the model in better training, aslo ensuring the model to be more generalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unOrrRcFXYsR"
      },
      "source": [
        "def split_date(df):\n",
        "  df['PurchDate'] = pd.to_datetime(df['PurchDate'])\n",
        "  df['Year'] = df.PurchDate.dt.year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RZXea0UqlC_"
      },
      "source": [
        "def MeanOnFeatures(df):\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_Make']=train.groupby(['Make'])['MMRCurrentAuctionAveragePrice'].transform('mean')\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_Model']=train.groupby(['Model'])['MMRCurrentAuctionAveragePrice'].transform('mean')\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_Trim']=train.groupby(['Trim'])['MMRCurrentAuctionAveragePrice'].transform('mean')\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_SubModel']=train.groupby(['SubModel'])['MMRCurrentAuctionAveragePrice'].transform('mean')\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_Color']=train.groupby(['Color'])['MMRCurrentAuctionAveragePrice'].transform('mean')\n",
        "  df['mean_MMRCurrentAuctionAveragePrice_Transmission']=train.groupby(['Transmission'])['MMRCurrentAuctionAveragePrice'].transform('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxmRcEqnkGg0"
      },
      "source": [
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3KwbqjCX0-8"
      },
      "source": [
        "split_date(train)\n",
        "split_date(test)\n",
        "\n",
        "print(train.shape, test.shape)\n",
        "\n",
        "MeanOnFeatures(train)\n",
        "MeanOnFeatures(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2zdd5EZkmyw"
      },
      "source": [
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_o3pg1GGzTN"
      },
      "source": [
        "not_relevant_coulmns = [\"PurchDate\", \"WheelTypeID\", \"Model\", \"Trim\", \"SubModel\", \"VNZIP1\", \"VNST\", \"Make\", \"Color\"]\n",
        "\n",
        "def remove_features(df):\n",
        "  df.drop(not_relevant_coulmns, axis=1, inplace=True)\n",
        "\n",
        "remove_features(train)\n",
        "remove_features(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lf3BO8FZmFp"
      },
      "source": [
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEuhh4d7lbQL"
      },
      "source": [
        "**Handling NaN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ISExKm5mteB"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ae1VLCnLct"
      },
      "source": [
        "I observed one particular scenario where the categrical values are varying becuase of the case sensitive issue. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQK-YV_mnD-w"
      },
      "source": [
        "train.Transmission.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a0B6f4rnf2e"
      },
      "source": [
        "# This should reslove this difference\n",
        "train[\"Transmission\"].replace(\"Manual\", \"MANUAL\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gEi19yoTbp"
      },
      "source": [
        "The target values seems to be highly imbalance. This is not good for our machine learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEbfavixoSM2"
      },
      "source": [
        "train.IsBadBuy.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V0AYDFyoqzk"
      },
      "source": [
        "count_class_0, count_class_1 = train.IsBadBuy.value_counts()\n",
        "\n",
        "df_class_0 = train[train['IsBadBuy'] == 0]\n",
        "df_class_1 = train[train['IsBadBuy'] == 1]\n",
        "\n",
        "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "print(df_test_over.IsBadBuy.value_counts())\n",
        "\n",
        "df_test_over.IsBadBuy.value_counts().plot(kind='bar', title='Count (target)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhhJORWwyAsm"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>5: Data Cleaning, Spliting and Pipelines Implementation</b></font> <a name=\"5\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT1J7mOGjicW"
      },
      "source": [
        "We perform the following tasks under this section\n",
        "- Segregating the features and target.\n",
        "- Identifying the numberical and categorical columns.\n",
        "- Spliting the dataset between train and testing. \n",
        "- For numberical columns\n",
        "  - Used KNN Imputer to fill the missing values.\n",
        "  - MinMaxScaler() to normalize the numberical values.\n",
        "- For categorical columns\n",
        "  - Used Simple Imputer to fill up the missing values.\n",
        "  - OneHotEncoder to all the catogorical columns.\n",
        "- Finally all the above steps are put inside a **pipelines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc6BQ2VEISdV"
      },
      "source": [
        "#create X and y datasets for splitting \n",
        "X = df_test_over.drop([\"RefId\",'IsBadBuy'], axis=1)\n",
        "y = df_test_over['IsBadBuy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do3urfCvISao"
      },
      "source": [
        "all_features = X.columns\n",
        "all_features = all_features.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4n5eGrPISXl"
      },
      "source": [
        "numerical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n",
        "                     if dtype.kind in ['i','f']]\n",
        "categorical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n",
        "                     if dtype.kind not in ['i','f']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEu77sfyISU8"
      },
      "source": [
        "#import train_test_split library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,  y, test_size=0.3, random_state=42) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFzw9jnsLrgV"
      },
      "source": [
        "preprocessor = make_column_transformer(\n",
        "    \n",
        "    (make_pipeline(\n",
        "    SimpleImputer(strategy = 'median'),\n",
        "    # KNNImputer(n_neighbors=2, weights=\"uniform\"),\n",
        "    MinMaxScaler()), numerical_features),\n",
        "    \n",
        "    (make_pipeline(\n",
        "    SimpleImputer(strategy = 'constant', fill_value = 'missing'),\n",
        "    OneHotEncoder(categories = 'auto', handle_unknown = 'ignore')), categorical_features),\n",
        "    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bry30NBQLrXe"
      },
      "source": [
        "preprocessor_best = make_pipeline(preprocessor, \n",
        "                                  VarianceThreshold(), \n",
        "                                  SelectKBest(f_classif, k = 50)\n",
        "                                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW27vLx9yAdR"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>6: Implemneting Random Forest</b></font> <a name=\"6\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr0zamtiLrUx"
      },
      "source": [
        "RF_Model = make_pipeline(preprocessor_best, RandomForestClassifier(n_estimators = 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSnpK2mFwU-D",
        "outputId": "f8ee7b73-3df5-45da-da42-bf7a8acb5c0b"
      },
      "source": [
        "RF_Model.fit(X_train, y_train)\n",
        "RF_Model.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBG5nXYFukYm"
      },
      "source": [
        "The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation/test set too.\n",
        "\n",
        "We can make predictions and compute accuracy in one step using model.score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZTInThNwU5Y",
        "outputId": "d583393b-7117-411e-e303-bc54bcf374c3"
      },
      "source": [
        "RF_Model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984324957687801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLBIH49vKVX"
      },
      "source": [
        "It appears that the model has learned the training examples perfect, and doesn't generalize well to previously unseen examples. One possible reason might be because we to reslove oversampling we ended up inducing some overlaps between testing and training set hence we are observing a very high accuracy. This phenomenon is called \"overfitting\", and reducing overfitting is one of the most important parts of any machine learning project.\n",
        "\n",
        "I can now think of two possible solutions\n",
        "1. hyperparamterization to over come the overfitting. I will cover this in next section.\n",
        "2. Instead of spliting the data in train and testing set. Let us train on the entire set in one go this valiation strategy is called as K-fold cross validation. To illustrate this via an example I will implement XGboost an apply cross validation on top on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3wHMTNyAHN"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>7: Hyperparamterization and Parameter Tuning  - Random Forest</b></font> <a name=\"7\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38eEPocJyIXQ"
      },
      "source": [
        "As we saw in the previous section, our random tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we'll look at some strategies for reducing overfitting. The process of reducing overfitting is known as regularlization.\n",
        "\n",
        "\n",
        "By varying the following fields, we can prevent the tree from memorizing all training examples, which may lead to better generalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raO48pbcLrSO"
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 19)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "#Maximum number of levels in tree\n",
        "max_depth = [2,4,6,8]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz-lCtqwLrP4",
        "outputId": "43c24e43-762b-4495-e6aa-c06ebdddc5e6"
      },
      "source": [
        "# Create the param grid\n",
        "param_grid = {'randomforestclassifier__n_estimators': n_estimators,\n",
        "               'randomforestclassifier__max_features': max_features,\n",
        "               'randomforestclassifier__max_depth': max_depth,\n",
        "               'randomforestclassifier__min_samples_split': min_samples_split,\n",
        "               'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n",
        "               'randomforestclassifier__bootstrap': bootstrap\n",
        "             }\n",
        "print(param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'randomforestclassifier__n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000], 'randomforestclassifier__max_features': ['auto', 'sqrt'], 'randomforestclassifier__max_depth': [2, 4, 6, 8], 'randomforestclassifier__min_samples_split': [2, 5], 'randomforestclassifier__min_samples_leaf': [1, 2], 'randomforestclassifier__bootstrap': [True, False]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNikJDEwLrM7"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf_RandomGrid = RandomizedSearchCV(estimator = RF_Model, param_distributions = param_grid, cv = 3, verbose=1, n_jobs = -1, n_iter = 5, scoring = 'f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ceRkvMJLrJ4",
        "outputId": "06ea18e5-74f3-4042-99db-ce763a6974bc"
      },
      "source": [
        "%%time\n",
        "rf_RandomGrid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "CPU times: user 1min 37s, sys: 1.19 s, total: 1min 38s\n",
            "Wall time: 9min 48s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=Pipeline(steps=[('pipeline',\n",
              "                                              Pipeline(steps=[('columntransformer',\n",
              "                                                               ColumnTransformer(transformers=[('pipeline-1',\n",
              "                                                                                                Pipeline(steps=[('simpleimputer',\n",
              "                                                                                                                 SimpleImputer(strategy='median')),\n",
              "                                                                                                                ('minmaxscaler',\n",
              "                                                                                                                 MinMaxScaler())]),\n",
              "                                                                                                ['VehYear',\n",
              "                                                                                                 'VehicleAge',\n",
              "                                                                                                 'VehOdo',\n",
              "                                                                                                 'MMRAcquisitionAuctionAveragePrice',\n",
              "                                                                                                 'MMRAcquisitionAuctionCleanPrice',\n",
              "                                                                                                 '...\n",
              "                   param_distributions={'randomforestclassifier__bootstrap': [True,\n",
              "                                                                              False],\n",
              "                                        'randomforestclassifier__max_depth': [2,\n",
              "                                                                              4,\n",
              "                                                                              6,\n",
              "                                                                              8],\n",
              "                                        'randomforestclassifier__max_features': ['auto',\n",
              "                                                                                 'sqrt'],\n",
              "                                        'randomforestclassifier__min_samples_leaf': [1,\n",
              "                                                                                     2],\n",
              "                                        'randomforestclassifier__min_samples_split': [2,\n",
              "                                                                                      5],\n",
              "                                        'randomforestclassifier__n_estimators': [100,\n",
              "                                                                                 150,\n",
              "                                                                                 200,\n",
              "                                                                                 250,\n",
              "                                                                                 300,\n",
              "                                                                                 350,\n",
              "                                                                                 400,\n",
              "                                                                                 450,\n",
              "                                                                                 500,\n",
              "                                                                                 550,\n",
              "                                                                                 600,\n",
              "                                                                                 650,\n",
              "                                                                                 700,\n",
              "                                                                                 750,\n",
              "                                                                                 800,\n",
              "                                                                                 850,\n",
              "                                                                                 900,\n",
              "                                                                                 950,\n",
              "                                                                                 1000]},\n",
              "                   scoring='f1', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGipvOuhuI1K",
        "outputId": "c94a014e-3bcf-46a1-9b06-d4fde45e7686"
      },
      "source": [
        "rf_RandomGrid.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7043851543401215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8bjj5mPuIyT",
        "outputId": "71bd4815-930e-4778-a63d-f9e51e1c04b0"
      },
      "source": [
        "rf_RandomGrid.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6884903169014085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfVtPYQ-y__K"
      },
      "source": [
        "Although the accuracy of overall model is reduced, We have significantly reduced overfitting as we see the co relation between the training and testing results. I now pick the best parameters which would help to me to achive better accuray from the avalaible list of parameters. \n",
        "\n",
        "Sklearn provides us the library **gridsearch CV** to help us running all the model over the list of parameter instead we doing it manually and **best_estimator** helps us in selecting the best parameter on which the model would befrom best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUJtHLssQE2j",
        "outputId": "cb6167d5-bb84-4fe7-f862-77c75afade72"
      },
      "source": [
        "rf_RandomGrid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('pipeline',\n",
              "                 Pipeline(steps=[('columntransformer',\n",
              "                                  ColumnTransformer(transformers=[('pipeline-1',\n",
              "                                                                   Pipeline(steps=[('simpleimputer',\n",
              "                                                                                    SimpleImputer(strategy='median')),\n",
              "                                                                                   ('minmaxscaler',\n",
              "                                                                                    MinMaxScaler())]),\n",
              "                                                                   ['VehYear',\n",
              "                                                                    'VehicleAge',\n",
              "                                                                    'VehOdo',\n",
              "                                                                    'MMRAcquisitionAuctionAveragePrice',\n",
              "                                                                    'MMRAcquisitionAuctionCleanPrice',\n",
              "                                                                    'MMRAcquisitionRetailAveragePrice',...\n",
              "                                                                                    OneHotEncoder(handle_unknown='ignore'))]),\n",
              "                                                                   ['Auction',\n",
              "                                                                    'Transmission',\n",
              "                                                                    'WheelType',\n",
              "                                                                    'Nationality',\n",
              "                                                                    'Size',\n",
              "                                                                    'TopThreeAmericanName',\n",
              "                                                                    'PRIMEUNIT',\n",
              "                                                                    'AUCGUART'])])),\n",
              "                                 ('variancethreshold', VarianceThreshold()),\n",
              "                                 ('selectkbest', SelectKBest(k=50))])),\n",
              "                ('randomforestclassifier',\n",
              "                 RandomForestClassifier(bootstrap=False, max_depth=8,\n",
              "                                        min_samples_leaf=2, min_samples_split=5,\n",
              "                                        n_estimators=500))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WATzeDMrQLKO"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq163eH1QEzC",
        "outputId": "001a54a7-eda8-4584-e6fb-d95521ededee"
      },
      "source": [
        "print(f'Train : {rf_RandomGrid.score(X_train, y_train):.3f}')\n",
        "print(f'Test : {rf_RandomGrid.score(X_test, y_test):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train : 0.704\n",
            "Test : 0.688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCzFJQIdznB2"
      },
      "source": [
        "It is observed that although the accuray has dropped significantly but the deviation between the training and test sets is minimal. Hence our model is fairly generalized.\n",
        "\n",
        "Results captured can now be submitted to kaggle. In the following block we create the output file and the same can be uploaded to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMvT0wOszCe3",
        "outputId": "5d501f2e-f040-44fa-cbe3-3d7c665e7295"
      },
      "source": [
        "preds = rf_RandomGrid.predict(test.iloc[:, 1:31])\n",
        "output = pd.DataFrame({'Refid': test.RefId, 'IsBadBuy': preds})\n",
        "output.to_csv('my_submission_rf_model_hyperparameterized.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your submission was successfully saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO2d1hvCyAOF"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>8: Implementing XBboost Forest</b></font> <a name=\"8\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9nfCZGC7Nvm"
      },
      "source": [
        "First we will setup our pipelines as usual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhcIVRnHLnVM"
      },
      "source": [
        "preprocessor = make_column_transformer(\n",
        "    \n",
        "    (make_pipeline(\n",
        "    #SimpleImputer(strategy = 'median'),\n",
        "    KNNImputer(n_neighbors=2, weights=\"uniform\"),\n",
        "    MinMaxScaler()), numerical_features),\n",
        "    \n",
        "    (make_pipeline(\n",
        "    SimpleImputer(strategy = 'constant', fill_value = 'missing'),\n",
        "    OneHotEncoder(categories = 'auto', handle_unknown = 'ignore')), categorical_features),\n",
        "\n",
        ")\n",
        "\n",
        "preprocessor_best = make_pipeline(preprocessor,\n",
        "                                  VarianceThreshold(), \n",
        "                                  SelectKBest(f_classif, k = 50)\n",
        "                                 )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hhGlSAG7YrF"
      },
      "source": [
        "Here we call the xgboost classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq4dQTb9r6tp"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "# model = XGBClassifier(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4, use_label_encoder=False)\n",
        "\n",
        "XG_model = make_pipeline(preprocessor_best, XGBClassifier(n_estimators = 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r2v4dSt9ido"
      },
      "source": [
        "Now we fit the model to the entire dataset. Remember we wont be performing train test split instead we will use K fold cross validation to evaluta our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6nWXHv9s2zr",
        "outputId": "94dd7063-7f2c-4a3d-dbe5-da09c922bcf5"
      },
      "source": [
        "%%time\n",
        "\n",
        "# XG_model.fit(X_train, y_train)\n",
        "XG_model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:07:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "CPU times: user 2min 3s, sys: 4.45 s, total: 2min 8s\n",
            "Wall time: 1min 32s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('pipeline',\n",
              "                 Pipeline(steps=[('columntransformer',\n",
              "                                  ColumnTransformer(transformers=[('pipeline-1',\n",
              "                                                                   Pipeline(steps=[('knnimputer',\n",
              "                                                                                    KNNImputer(n_neighbors=2)),\n",
              "                                                                                   ('minmaxscaler',\n",
              "                                                                                    MinMaxScaler())]),\n",
              "                                                                   ['VehYear',\n",
              "                                                                    'VehicleAge',\n",
              "                                                                    'VehOdo',\n",
              "                                                                    'MMRAcquisitionAuctionAveragePrice',\n",
              "                                                                    'MMRAcquisitionAuctionCleanPrice',\n",
              "                                                                    'MMRAcquisitionRetailAveragePrice',\n",
              "                                                                    'MMRAcquis...\n",
              "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
              "                               importance_type='gain',\n",
              "                               interaction_constraints='',\n",
              "                               learning_rate=0.300000012, max_delta_step=0,\n",
              "                               max_depth=6, min_child_weight=1, missing=nan,\n",
              "                               monotone_constraints='()', n_estimators=100,\n",
              "                               n_jobs=2, num_parallel_tree=1, random_state=0,\n",
              "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
              "                               subsample=1, tree_method='exact',\n",
              "                               validate_parameters=1, verbosity=None))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fFTdcueMdWC",
        "outputId": "ff367cc2-2a85-4b28-c74e-720b12a685c7"
      },
      "source": [
        "# XG_model.score(X_train, y_train)\n",
        "XG_model.score(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8396972206164951"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFtYWkCD909t"
      },
      "source": [
        "Our model has shown the accuracy of about 83% when tested on train set. Let us now implement the k fold cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2foFqR8Kyk5o"
      },
      "source": [
        "**K Fold Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0i9qEgyoBn"
      },
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IckEakfyn-d"
      },
      "source": [
        "def train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n",
        "    model = make_pipeline(preprocessor_best, XGBClassifier(random_state=42, n_jobs=-1, **params))\n",
        "    model.fit(X_train, train_targets)\n",
        "    train_accuracy = model.score(X_train, train_targets)\n",
        "    val_accuracy = model.score(X_val, val_targets)\n",
        "    return model, train_accuracy, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocf3yooDyn8J"
      },
      "source": [
        "kfold = KFold(n_splits=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRGRn0Yvyn5G",
        "outputId": "b2ffe1d0-ee5b-4e7a-ccc4-ea3334497b4f"
      },
      "source": [
        "models = []\n",
        "\n",
        "for train_idxs, val_idxs in kfold.split(X):\n",
        "    X_train, train_targets = X.iloc[train_idxs], y.iloc[train_idxs]\n",
        "    X_val, val_targets = X.iloc[val_idxs], y.iloc[val_idxs]\n",
        "    model, train_accuracy, val_accuracy = train_and_evaluate(X_train, \n",
        "                                                     train_targets, \n",
        "                                                     X_val, \n",
        "                                                     val_targets, \n",
        "                                                     max_depth=4, \n",
        "                                                     n_estimators=20)\n",
        "    models.append(model)\n",
        "    print('Train Accuracy: {}, Validation Accuracy: {}'.format(train_accuracy, val_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:14:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Train Accuracy: 0.7315815683862085, Validation Accuracy: 0.3719876576963637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:16:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Train Accuracy: 0.7172081124097998, Validation Accuracy: 0.3719876576963637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:18:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Train Accuracy: 0.7171397603773032, Validation Accuracy: 0.4797484669765262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:19:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Train Accuracy: 0.7364931501498863, Validation Accuracy: 0.41510760457758855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10:21:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "Train Accuracy: 0.7382533296879272, Validation Accuracy: 0.4253183345051168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCag7rjB287C"
      },
      "source": [
        "Train Accuracy: 0.7321576783743934, Validation Accuracy: 0.3810881537319845\n",
        "\n",
        "Train Accuracy: 0.7148939078809894, Validation Accuracy: 0.39284458852478227\n",
        "\n",
        "Train Accuracy: 0.7126871136889592, Validation Accuracy: 0.48498222864508067\n",
        "\n",
        "Train Accuracy: 0.7391100565368954, Validation Accuracy: 0.43030113658555635\n",
        "\n",
        "Train Accuracy: 0.7381556848806781, Validation Accuracy: 0.4356690883524725\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0thXDSW3qfc"
      },
      "source": [
        "As observed here although the accuracy of training set is high but validation set seems to exbhit very low accuracy. So none of these model is good enough but we can take avaerage of these model so that the errors are reduced. Let's also define a function to average predictions from the 5 different models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9gTZ5x6yn2b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_avg(models, inputs):\n",
        "    return np.mean([model.predict(inputs) for model in models], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD2h1YGH-s65"
      },
      "source": [
        "Now we predict the test outcome based on the average learning of all above 5 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65z5Vs6T1uJv"
      },
      "source": [
        "preds = predict_avg(models, test.iloc[:, 1:31])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh_qWKQc1t-B"
      },
      "source": [
        "for x in range(0, len(preds)):\n",
        "  if preds[x] <=0.5:\n",
        "    preds[x] =0\n",
        "  else:\n",
        "    preds[x] =1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdDMGxGr1uBH",
        "outputId": "645ddde5-b0ab-420f-88eb-07ec90303cc8"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv9PMVUHEpai"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>9: Hyperparamterization and Parameter Tuning  - XGboost</b></font> <a name=\"9\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL603s6J3yvc"
      },
      "source": [
        "By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TucrJR5OoyN",
        "outputId": "856275cd-8d78-4432-aba6-484de8ce18d8"
      },
      "source": [
        "param_grid = {\n",
        "# Number of trees in random forest\n",
        "'xgbclassifier__n_estimators' : [int(x) for x in np.linspace(start = 100, stop = 1000, num = 100)],\n",
        "# Number of features to consider at every split\n",
        "'xgbclassifier__learning_rate' : [0.1, 0.2, 0.3],\n",
        "#Maximum number of levels in tree\n",
        "'xgbclassifier__max_depth' : [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "print(param_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'xgbclassifier__n_estimators': [100, 109, 118, 127, 136, 145, 154, 163, 172, 181, 190, 200, 209, 218, 227, 236, 245, 254, 263, 272, 281, 290, 300, 309, 318, 327, 336, 345, 354, 363, 372, 381, 390, 400, 409, 418, 427, 436, 445, 454, 463, 472, 481, 490, 500, 509, 518, 527, 536, 545, 554, 563, 572, 581, 590, 600, 609, 618, 627, 636, 645, 654, 663, 672, 681, 690, 700, 709, 718, 727, 736, 745, 754, 763, 772, 781, 790, 800, 809, 818, 827, 836, 845, 854, 863, 872, 881, 890, 900, 909, 918, 927, 936, 945, 954, 963, 972, 981, 990, 1000], 'xgbclassifier__learning_rate': [0.1, 0.2, 0.3], 'xgbclassifier__max_depth': [2, 4, 6, 8]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Yyox_3Oove"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "XG_RandomGrid = RandomizedSearchCV(estimator = XG_model, param_distributions = param_grid, cv = 3, verbose=1, n_jobs = -1, n_iter = 5, scoring = 'f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCywwCheOos9",
        "outputId": "c25aafe9-ee6e-4f5d-ed50-e9c21ee1cc7c"
      },
      "source": [
        "%%time\n",
        "# XG_RandomGrid.fit(X_train, y_train)\n",
        "XG_RandomGrid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT1-CoPGOoqM"
      },
      "source": [
        "# XG_model.best_estimator_\n",
        "XG_RandomGrid.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0tIREcdOon6"
      },
      "source": [
        "# print(f'Train : {XG_RandomGrid.score(X_train, y_train):.3f}')\n",
        "print(f'Train : {XG_RandomGrid.score(X, y):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5BRNL4YENlO"
      },
      "source": [
        "# print(f'Test : {XG_RandomGrid.score(X_test, y_test):.3f}')\n",
        "print(f'Test : {XG_RandomGrid.score(X_val, val_targets):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAOXe2bIG6dR"
      },
      "source": [
        "preds = XG_RandomGrid.predict(test.iloc[:, 1:31])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCoX6EV1G6Wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8c98bf-5c1e-4758-d438-cd6063f8f926"
      },
      "source": [
        "output = pd.DataFrame({'Refid': test.RefId, 'IsBadBuy': preds})\n",
        "output.to_csv('my_submission_xgboost_hypermeterizedwithout_featuresEng.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your submission was successfully saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3xAP_Wzx_-9"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>8: Sample prediction and saving model weights</b></font> <a name=\"8\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycvXS4qmE9dG"
      },
      "source": [
        "Making Predictions on New Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T0cKi0LE_Bz"
      },
      "source": [
        "def predict_input(model, single_input):\n",
        "    pred = XG_RandomGrid.predict(X_input)[0]\n",
        "    prob = XG_RandomGrid.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
        "    return pred, prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHUmoMdbFX2A"
      },
      "source": [
        "new_input = {'Date': '2021-06-19',\n",
        "             'Location': 'Launceston',\n",
        "             'MinTemp': 23.2,\n",
        "             'MaxTemp': 33.2,\n",
        "             'Rainfall': 10.2,\n",
        "             'Evaporation': 4.2,\n",
        "             'Sunshine': np.nan,\n",
        "             'WindGustDir': 'NNW',\n",
        "             'WindGustSpeed': 52.0,\n",
        "             'WindDir9am': 'NW',\n",
        "             'WindDir3pm': 'NNE',\n",
        "             'WindSpeed9am': 13.0,\n",
        "             'WindSpeed3pm': 20.0,\n",
        "             'Humidity9am': 89.0,\n",
        "             'Humidity3pm': 58.0,\n",
        "             'Pressure9am': 1004.8,\n",
        "             'Pressure3pm': 1001.5,\n",
        "             'Cloud9am': 8.0,\n",
        "             'Cloud3pm': 5.0,\n",
        "             'Temp9am': 25.7,\n",
        "             'Temp3pm': 33.0,\n",
        "             'RainToday': 'Yes'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzmcrGAIFdKC"
      },
      "source": [
        "predict_input(XG_RandomGrid, new_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeG_pAvSEljC"
      },
      "source": [
        "We can save the parameters (weights and biases) of our trained model to disk, so that we needn't retrain the model from scratch each time we wish to use it. Along with the model, it's also important to save imputers, scalers, encoders and even column names. Anything that will be required while generating predictions using the model should be saved.\n",
        "\n",
        "We can use the joblib module to save and load Python objects on the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1siYCAoB7px"
      },
      "source": [
        "import joblib\n",
        "\n",
        "car_quality_check = {\n",
        "    'model': XG_RandomGrid\n",
        "}\n",
        "\n",
        "joblib.dump(car_quality_check, 'car_quality_check.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOh-QD3vCiOj"
      },
      "source": [
        "The object can be loaded back using joblib.load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-1gtSZClzs"
      },
      "source": [
        "car_quality_check2 = joblib.load('car_quality_check.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4El89NajC0Nc"
      },
      "source": [
        "We can check if the reloaded model is bheaving as we expected it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGSgHyGC56D"
      },
      "source": [
        "test_preds2 = car_quality_check2['model'].predict(X_test)\n",
        "accuracy_score(test_targets, test_preds2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YbQFGixywkp"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>9: Summary</b></font> <a name=\"9\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOWQ28gpJG-"
      },
      "source": [
        "I summarize the my entire notebook as follows\n",
        "\n",
        "- We downloaded the Car quality detection Dataset dataset from Kaggle.\n",
        "- We ran EDA and analysised the input features.\n",
        "- We then performed feature engineering and data cleaning to filtered out on the relevant user data.\n",
        "- After this we take two different algorith to build Machine learning model.\n",
        "  - Random forest\n",
        "  - XGboost\n",
        "- We applied hyperparameterization and parameter tuning to get best our of the ml model and to generalize it in best possible ways.\n",
        "- The output file generate can be submitted to Kaggle to evaluate your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZeVlRBKywfh"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>10: Future Work</b></font> <a name=\"10\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2_4E4scqXBj"
      },
      "source": [
        "- Although I tried couple of models. There are many more models which can be tried off like decision trees, light GMS\n",
        "\n",
        "- Implementing deep learning to get better model\n",
        "\n",
        "- sumbit your results to Kaggle competions and evaluate your model perfomace at the leader boad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HTo8gG0ywc6"
      },
      "source": [
        "# <font color=\"white\" size=+2.0><b>11: References</b></font> <a name=\"11\"></a>\n",
        "\n",
        "\n",
        "---\n",
        "<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EibMu-9rHaF"
      },
      "source": [
        "I took my innspriation from the following notebooks\n",
        "\n",
        "- https://www.kaggle.com/mikhailtokar/ml-rgr-dgk\n",
        "- https://www.kaggle.com/funxexcel/starter-code-don-t-get-kicked-rf-model\n",
        "- https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms/lesson/random-forests-and-regularization\n",
        "- https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms/lesson/gradient-boosting-with-xgboost\n",
        "\n",
        "Lastly, I would like to say thanks to Akansh and the Jovian.ml for providing the course on machine learning. If u want to get started this [course](https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms) might be good start."
      ]
    }
  ]
}